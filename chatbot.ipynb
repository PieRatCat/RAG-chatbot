{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5204dee",
   "metadata": {},
   "source": [
    "# RAG chatbot project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cb11a3",
   "metadata": {},
   "source": [
    "**Disclaimer: Educational Use and Copyright Notice**\n",
    "\n",
    "This chatbot is a non-commercial, educational project developed as part of the \"DS24 Deep Learning\" course requirements. The knowledge base for this chatbot was created using content from official Dragon Age game guides for research and demonstration purposes.\n",
    "\n",
    "The use of this copyrighted material is for educational purposes only. This project is not intended for any commercial use, and there is no financial gain derived from its creation or operation. All copyrights and intellectual property rights for the Dragon Age franchise, its characters, lore, and related materials belong to their respective owners, primarily BioWare and Electronic Arts (EA).\n",
    "\n",
    "No copyright infringement is intended. This project is a demonstration of the technical implementation of a Retrieval Augmented Generation (RAG) system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ee5ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# AUTHOR: Amanda Sumner\n",
    "# COURSE: DS24 Deep Learning - Kunskapskontroll 2\n",
    "# DATE: May 25, 2025\n",
    "#\n",
    "# PROJECT: RAG Chatbot for Dragon Age Lore\n",
    "#\n",
    "# DESCRIPTION: This project implements a Retrieval Augmented Generation (RAG)\n",
    "# chatbot designed to answer questions about the Dragon Age universe. The\n",
    "# knowledge base is sourced from text-based PDF game guides. The chatbot\n",
    "# uses the LangChain framework for the pipeline, Google's Gemini\n",
    "# models for text embedding and generation, and ChromaDB as a persistent\n",
    "# vector store for retrieval. The application is presented through\n",
    "# a Streamlit web interface and adopts the persona of a Dragon Age NPC, \n",
    "# Chantry scholar Brother Genitivi, for its responses.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a0b5696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657bffa0",
   "metadata": {},
   "source": [
    "## LangChain\n",
    "\n",
    "LangChain acts as a framework that simplifies and standardises the process of building an application with Large Language Model (LLM). It speeds up development and reduces complexity and is a more modern approach for a project rather than the alternative of using independent libraries and connecting each component from scratch.  \n",
    "For a RAG chatbot I need to connect several different components: a document loader, a text splitter, a vector store, a retriever, a prompt template, and the LLM. LangChain is the framework to connect all these pieces in a RAG chain.  \n",
    "LangChain provides prebuilt libraries which removes the necessity to write low-level code for each component and makes it easy to swap out a component such as vector store or LLM without having to rewrite the entire chain.  \n",
    "RAG chain defines the flow of data: the user's question to retriever -> the retrieved context to the prompt -> the prompt to the LLM -> the LLM's output is parsed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4633fd",
   "metadata": {},
   "source": [
    "### LangSmith \n",
    "\n",
    "Loading LangSmith project to use with LangChain <https://smith.langchain.com/>  \n",
    "LangSmith is the platform for building LLM applications that allows to monitor and evaluate applications.   \n",
    "Here I enable tracing for project \"chatbot\" mainly to track the app runs and LLM calls to monitor the API token count and cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59271f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "if \"LANGSMITH_API_KEY\" not in os.environ:\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\n",
    "        prompt=\"Enter your LangSmith API key (optional): \"\n",
    "    )\n",
    "LANGSMITH_PROJECT_NAME = \"chatbot\"\n",
    "if os.environ.get(\"LANGSMITH_TRACING\") == \"true\" and os.environ.get(\"LANGSMITH_API_KEY\"):\n",
    "    os.environ[\"LANGSMITH_PROJECT\"] = LANGSMITH_PROJECT_NAME\n",
    "elif \"LANGSMITH_PROJECT\" in os.environ:\n",
    "    del os.environ[\"LANGSMITH_PROJECT\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03fc9d3",
   "metadata": {},
   "source": [
    "### Reading PDF files from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46031089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:40<00:00, 11.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-*- Successfully loaded a total of 1929 pages from all PDFs. -*-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_directory_path = \"files/\"\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    pdf_directory_path,\n",
    "    glob=\"*.pdf\", \n",
    "    loader_cls=PyPDFLoader,\n",
    "    show_progress=True,\n",
    "    use_multithreading=True\n",
    "    )\n",
    "\n",
    "all_loaded_pages = loader.load()\n",
    "\n",
    "print(f\"\\n-*- Successfully loaded a total of {len(all_loaded_pages)} pages from all PDFs. -*-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5363cf08",
   "metadata": {},
   "source": [
    "### Verification and quality check\n",
    "\n",
    "Checking that all files have loaded and how many pages were loaded from each file.   \n",
    "Then checking for empty pages.   \n",
    "Finally, displaying a snippet from the middle page in each document to verify that text has loaded properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7aa6097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-*- Verification of Loaded Documents -*-\n",
      "\n",
      "[INFO] Page count per source file:\n",
      "source_file\n",
      "DAIprimaguide.pdf    355\n",
      "DAOguide2.pdf        338\n",
      "CodexDAI.pdf         310\n",
      "DAIguide.pdf         216\n",
      "CodexDAO.pdf         177\n",
      "DAOguide.pdf         152\n",
      "DA2guide.pdf         136\n",
      "CodexDA2.pdf         132\n",
      "DAOAguide.pdf        113\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[WARNING] Found empty pages in the following files:\n",
      "           source_file  page_number  content_length content_snippet\n",
      "722  DAIprimaguide.pdf           13               0             ...\n",
      "843  DAIprimaguide.pdf          134               0             ...\n",
      "844  DAIprimaguide.pdf          135               0             ...\n",
      "881  DAIprimaguide.pdf          172               0             ...\n",
      "886  DAIprimaguide.pdf          177               0             ...\n",
      "903  DAIprimaguide.pdf          194               0             ...\n"
     ]
    }
   ],
   "source": [
    "# Verification and quality check\n",
    "if 'all_loaded_pages' in locals() and all_loaded_pages:\n",
    "    # Creating a list of dictionaries, where each dict represents a page\n",
    "    page_data = []\n",
    "    for page_doc in all_loaded_pages:\n",
    "        page_data.append({\n",
    "            \"source_file\": os.path.basename(page_doc.metadata.get('source', 'N/A')), # Get the filename\n",
    "            \"page_number\": page_doc.metadata.get('page', -1) + 1, # Page numbers are 0-indexed\n",
    "            \"content_length\": len(page_doc.page_content),\n",
    "            \"content_snippet\": page_doc.page_content[:300].replace('\\n', ' ') + \"...\" # A text snippet\n",
    "        })\n",
    "    \n",
    "    # Creating a pandas DataFrame\n",
    "    df = pd.DataFrame(page_data)\n",
    "\n",
    "    print(\"\\n-*- Verification of Loaded Documents -*-\")\n",
    "\n",
    "    # 1. Checking if all PDFs were loaded and how many pages from each file\n",
    "    print(\"\\n[INFO] Page count per source file:\")\n",
    "    print(df['source_file'].value_counts())\n",
    "\n",
    "    # 2. Checking for empty pages (content_length == 0)\n",
    "    empty_pages = df[df['content_length'] == 0]\n",
    "    if not empty_pages.empty:\n",
    "        print(\"\\n[WARNING] Found empty pages in the following files:\")\n",
    "        print(empty_pages)\n",
    "    else:\n",
    "        print(\"\\n[INFO] No empty pages found. All loaded pages have content.\")\n",
    "\n",
    "else:\n",
    "    print(\"Warning: 'all_loaded_pages' is not defined or is empty. Run the PDF loading block first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89e36684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-*- Checking snippets from: DA2guide.pdf -*-\n",
      "  Snippet from page 69:\n",
      "    'Dragon Age II Dragon Age II  Official Digital Strategy Guide Official Digital Strategy Guide    Staves Staves deal damage at range with bolts of energy and up close with physical blows (though that may not suit the disposition of most mages). They can score critical hits with their basic attacks lik...'\n",
      "\n",
      "-*- Checking snippets from: DAOAguide.pdf -*-\n",
      "  Snippet from page 57:\n",
      "    'Dragon Age: Origins – Awakening Dragon Age: Origins – Awakening  Official Digital Strategy Guide Official Digital Strategy Guide    for PC, PS3, Xbox 360for PC, PS3, Xbox 360 Grey Warden Companions If you thought the companions who rallied with you against the archdemon were a fascinating lot, wait ...'\n",
      "\n",
      "-*- Checking snippets from: DAOguide.pdf -*-\n",
      "  Snippet from page 77:\n",
      "    'Dragon Age: Origins Dragon Age: Origins  Official Digital Strategy Guide Official Digital Strategy Guide    for PC, PS3, Xbox 360for PC, PS3, Xbox 360 The party's healer will save or damn a group when an encounter gets hot and heavy. As a healer, you have to know when to launch your Group Heal, when...'\n",
      "\n",
      "-*- Checking snippets from: CodexDAO.pdf -*-\n",
      "  Snippet from page 89:\n",
      "    'When her only son began to show signs of possessing magic, Isolde tried to cover it up, knowing that he would be taken from her by the Circle if found out. She hired an apostate mage to tutor him in secret, little knowing that her tutor was being paid to poison her husband. Eamon fell ill, and Conno...'\n",
      "\n",
      "-*- Checking snippets from: CodexDA2.pdf -*-\n",
      "  Snippet from page 67:\n",
      "    'Of course, the greatest consumer of slave labor is the Tevinter Imperium, which would surely crumble if not for the endless supply of slaves from all over the continent. There, they are meat, chattel. They are beaten, used as fodder in the endless war against the Qunari, and even serve as components...'\n",
      "\n",
      "-*- Checking snippets from: DAIprimaguide.pdf -*-\n",
      "  Snippet from page 178:\n",
      "    'PRIMA OFFICIAL GAME GUIDE  I rrrwix  ♦ Dragon Piss c. Granite Point  4r The Den  4 Old Windmill  Song Lyrics  Glyphs  Solas Artifacts...'\n",
      "\n",
      "-*- Checking snippets from: DAIguide.pdf -*-\n",
      "  Snippet from page 109:\n",
      "    'Dragon Age: Inquisition Dragon Age: Inquisition  Official Digital Strategy Guide Official Digital Strategy Guide    for PC, PS3, PS4, Xbox 360, Xbox Onefor PC, PS3, PS4, Xbox 360, Xbox One Requisition Quests Requisition Materials Coat Requisition in the Emprise 5 Plush Fustian Velvet, 10 Snoufleur S...'\n",
      "\n",
      "-*- Checking snippets from: CodexDAI.pdf -*-\n",
      "  Snippet from page 156:\n",
      "    'In the rest of the civilized world, it is common belief that Antiva has no king. I assure you, gentle readers, that this is untrue. The line of kings in Antiva has remained unbroken for two and a half thousand years—it is simply that nobody pays any attention to them whatsoever.  In truth, the natio...'\n",
      "\n",
      "-*- Checking snippets from: DAOguide2.pdf -*-\n",
      "  Snippet from page 170:\n",
      "    '170 PRIMA Official Game Guide Home City Elf Origin Home Cheatsheet Main Plot Quests • Life in the Alienage—A Day  for Celebration Important NPCs • Cyrion • Shianni Key Items • Adaia’s Boots • Wedding Clothes in Footlocker Monsters • None Side Quests • None Runthrough (Home) Summary: The player is a ...'\n"
     ]
    }
   ],
   "source": [
    "# Quality check: Display snippets from the middle of each document    \n",
    "# Get a list of the unique source files from the DataFrame\n",
    "source_files = df['source_file'].unique()\n",
    "    \n",
    "for source_file in source_files:\n",
    "    print(f\"\\n-*- Checking snippets from: {source_file} -*-\")\n",
    "        \n",
    "    # Get the subset of the DataFrame for the current file\n",
    "    df_file = df[df['source_file'] == source_file]\n",
    "                      \n",
    "    # Check a page from the middle of the document, if it exists\n",
    "    if len(df_file) > 2:\n",
    "        middle_page_index = len(df_file) // 2\n",
    "        print(f\"  Snippet from page {df_file.iloc[middle_page_index]['page_number']}:\")\n",
    "        print(f\"    '{df_file.iloc[middle_page_index]['content_snippet']}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e17ffc",
   "metadata": {},
   "source": [
    "### Chunking the retrieved text\n",
    "\n",
    "Chunk size 1000 and chunk overlap 200 are a common starting point for RAG systems, balancing the semantic context and retrieval precision. A chunk of 1000 characters contains approximately one or two complete paragraphs, ensuring that the immediate context is kept together. Considering the text format in the source documents, it is the right size to retrieve a paragraph with the answer to a specific question. Smaller chunk size could miss the point and context, and a too large chunk size would include irrelevant details.   \n",
    "Chunk overlap 200 is 20% of the chunk size and provides a safety margin to ensure that sentences are not cut off and meaning is not lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a82cd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the total content into 10491 chunks.\n",
      "\n",
      "-*- Document Chunking Complete -*-\n",
      "Variable 'all_chunks' is now ready for embedding and storage.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    "    )\n",
    "all_chunks = text_splitter.split_documents(all_loaded_pages)\n",
    "print(f\"Split the total content into {len(all_chunks)} chunks.\")\n",
    "\n",
    "print(\"\\n-*- Document Chunking Complete -*-\")\n",
    "print(\"Variable 'all_chunks' is now ready for embedding and storage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ea8379",
   "metadata": {},
   "source": [
    "### Embedding and storing data\n",
    "\n",
    "Google Gemini AI has several embedding models that can be used for semantic search, text classification, clustering, code retrieval, etc. When building a RAG system, text embeddings are used to measure the relatedness of strings and perform a semantic similarity search.   \n",
    "Currently the newest is the experimental Gemini embedding model `gemini-embedding-exp-03-07`. However, I chose a stable release model `text-embedding-004`. It is optimised for creating embeddings with 768 dimensions for text of up to 2048 tokens and has a rate limit of 1500 requests per minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7761c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fe741c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-*- Documents successfully embedded and stored. -*-\n",
      "Number of vectors/chunks in the store: 10491\n"
     ]
    }
   ],
   "source": [
    "vector_store = Chroma.from_documents(\n",
    "    documents=all_chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"chatbot_project\",\n",
    "    persist_directory=\"./chatbot_db\"\n",
    ")\n",
    "print(f\"\\n-*- Documents successfully embedded and stored. -*-\")\n",
    "print(f\"Number of vectors/chunks in the store: {vector_store._collection.count()}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6bf7e2",
   "metadata": {},
   "source": [
    "### RAG chain\n",
    "\n",
    "Setting up the RAG chain: \n",
    "* Google API and `gemini-2.0-flash` model for generation\n",
    "* retriever which will retrieve k number of chunks\n",
    "* prompt template\n",
    "* LangChain libraries to format the retrieved documents into a string and parse the output\n",
    "* returning `rag_chain` variable for using in queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9211a10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that the GOOGLE_API_KEY is set\n",
    "if \"GOOGLE_API_KEY\" not in os.environ or not os.environ[\"GOOGLE_API_KEY\"]:\n",
    "    print(\"Warning: GOOGLE_API_KEY not found or is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d3bda03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-*- Setting up the RAG Chain -*-\n",
      "LLM 'gemini-2.0-flash' initialized.\n",
      "Retriever created. Will retrieve 5 chunks.\n",
      "Prompt template created.\n",
      "\n",
      "-*- RAG chain constructed successfully! -*-\n",
      "The 'rag_chain' variable is now ready for queries.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n-*- Setting up the RAG Chain -*-\")\n",
    "\n",
    "# Initialising LLM for generation\n",
    "# Using Gemini 2.0 Flash model which is suitable for RAG tasks.\n",
    "GENERATION_MODEL_NAME = \"gemini-2.0-flash\"\n",
    "llm = ChatGoogleGenerativeAI(model=GENERATION_MODEL_NAME, temperature=0.3) # temperature controls creativity\n",
    "print(f\"LLM '{GENERATION_MODEL_NAME}' initialized.\")\n",
    "\n",
    "# Creating a retriever from the vector store\n",
    "# k is the number of documents to retrieve.\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "print(f\"Retriever created. Will retrieve {retriever.search_kwargs['k']} chunks.\")\n",
    "\n",
    "# Designing the prompt template\n",
    "# Persona prompt for Brother Genitivi (a character from Dragon Age lore)\n",
    "template = \"\"\"\n",
    "### ROLE & PERSONA ###\n",
    "You are Brother Ferdinand Genitivi, a renowned Chantry scholar, historian, and author from the world of Thedas.\n",
    "\n",
    "### TONE & STYLE ###\n",
    "- **Core Style:** Your tone is academic and formal, but filled with an eccentric and obsessive passion for history. \n",
    "You frame your answers as if documenting your findings for the historical record, blending the principles of a \"man of science and of God\".\n",
    "For mundane or irrelevant questions, you may be slightly dismissive.\n",
    "- **Vocabulary:** Use a rich, scholarly vocabulary (e.g., \"postulate,\" \"empirical,\" \"fallacious,\" \"persevere\") \n",
    "and naturally incorporate Chantry-specific terms like \"the Maker\" and \"the Chant of Light.\"\n",
    "- **Sentence Structure:** Employ complex sentences with multiple clauses, reflecting a thoughtful and detailed writing process.\n",
    "- **Rhetorical Approach:** Emphasize your role as a seeker of \"truth\" over superstition and dogma.\n",
    "- **First-Person Narrative:** Frame your knowledge through the lens of your personal experiences and scholarly struggles.\n",
    "\n",
    "\n",
    "### TASK ###\n",
    "Answer the user's question based *only* on the provided context below.\n",
    "- If the context contains the answer, synthesize it and respond in your persona as Brother Genitivi.\n",
    "- If the answer is not present in the context, you must state that the information is not within the texts you have at hand.\n",
    "- Do not, under any circumstances, make up an answer or use knowledge from outside the provided context.\n",
    "\n",
    "### CONTEXT ###\n",
    "{context}\n",
    "\n",
    "### QUESTION ###\n",
    "{question}\n",
    "\n",
    "### YOUR ANSWER ###\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "print(\"Prompt template created.\")\n",
    "\n",
    "# Constructing the RAG Chain using LangChain's RunnableParallel and RunnablePassthrough\n",
    "# Helper function to format retrieved documents into a single string\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)\n",
    "\n",
    "print(\"\\n-*- RAG chain constructed successfully! -*-\")\n",
    "print(\"The 'rag_chain' variable is now ready for queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e26f11",
   "metadata": {},
   "source": [
    "### Testing the RAG chain\n",
    "\n",
    "The test involves asking two questions: one that can be answered by using the text from the source documents and another that asks something from outside the provided context and that the chatbot should answer with a version of \"I don't know\", as prompted.   \n",
    "A successful test will correctly answer the first question and not make up an answer to the second question but instead reply that it can't answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea6aadf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-*- Testing the RAG chain -*-\n",
      "\n",
      "Invoking chain with question: 'When did the fifth blight start?'\n",
      "\n",
      "Question: When did the fifth blight start?\n",
      "Answer: Based on the texts at hand, I can confirm that the Fifth Blight began in the swamps of the Korcari Wilds on the southeastern border of Ferelden in the year 9:30 Dragon. It is a matter of historical record, though some contemporaries dispute whether it was a true Blight or merely a large darkspawn resurgence.\n",
      "\n",
      "Invoking chain with question: 'What is your favourite sandwich?'\n",
      "\n",
      "Question: What is your favourite sandwich?\n",
      "Answer: Alas, esteemed seeker of knowledge, within the texts at my disposal, there is no mention of my preferred sandwich. However, I have documented a superb dish of deboned fish, boiled eggs, dried fruit, spices, and thickened cream, all topped with a light crust. Perhaps this would be of interest to you instead?\n"
     ]
    }
   ],
   "source": [
    "# Testing the RAG chain\n",
    "if 'rag_chain' in locals() and rag_chain:\n",
    "    print(\"\\n-*- Testing the RAG chain -*-\")\n",
    "\n",
    "    # Test 1: A question that should be answerable from the source documents\n",
    "    test_question_relevant = \"When did the fifth blight start?\"\n",
    "   \n",
    "    print(f\"\\nInvoking chain with question: '{test_question_relevant}'\")\n",
    "    try:\n",
    "        response_relevant = rag_chain.invoke(test_question_relevant)\n",
    "        print(f\"\\nQuestion: {test_question_relevant}\")\n",
    "        print(f\"Answer: {response_relevant.get('answer', 'N/A - Answer key not found in response')}\")\n",
    "    except Exception as e_invoke_relevant:\n",
    "        print(f\"Error invoking RAG chain for relevant question: {type(e_invoke_relevant).__name__} - {e_invoke_relevant}\")\n",
    "\n",
    "    # Test 2: A question outside the context to check \"I don't know\" behavior\n",
    "    test_question_outside_context = \"What is your favourite sandwich?\"\n",
    "    print(f\"\\nInvoking chain with question: '{test_question_outside_context}'\")\n",
    "    try:\n",
    "        response_unknown = rag_chain.invoke(test_question_outside_context)\n",
    "        print(f\"\\nQuestion: {test_question_outside_context}\")\n",
    "        print(f\"Answer: {response_unknown.get('answer', 'N/A - Answer key not found in response')}\")\n",
    "    except Exception as e_invoke_unknown:\n",
    "        print(f\"Error invoking RAG chain for out-of-context question: {type(e_invoke_unknown).__name__} - {e_invoke_unknown}\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: 'rag_chain' is not defined. Please run RAG chain setup first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c9ec29",
   "metadata": {},
   "source": [
    "## Streamlit application\n",
    "\n",
    "### Overview of Streamlit Application Development for the RAG Chatbot\n",
    "\n",
    "This project concluded in the development of an interactive web application using Streamlit to provide a user-friendly interface for the Dragon Age lore RAG (Retrieval Augmented Generation) chatbot. The goal was to create an accessible platform where users could ask questions and receive in-character responses from \"Brother Genitivi\" based on the knowledge extracted from the provided game guide PDFs.   \n",
    "The app was deployed on Streamlit Cloud and is available at the following link: <https://da-chatbot.streamlit.app/>\n",
    "\n",
    "#### Key technologies and components used:\n",
    "\n",
    "Streamlit: Chosen as the framework for building the web application due to its ease of use, development capabilities, and Python-centric approach.    \n",
    "LangChain Components: The core RAG chain, previously constructed, was integrated. This involved:\n",
    "- Loading the pre-existing ChromaDB vector store (persisted in the ./chatbot_db directory).\n",
    "- Initialising the GoogleGenerativeAIEmbeddings model (`text-embedding-004`) for embedding queries.\n",
    "- Initialising the ChatGoogleGenerativeAI model (`gemini-2.0-flash`) for response generation.\n",
    "- Using the persona prompt for Brother Genitivi.\n",
    "\n",
    "Streamlit Features:\n",
    "- `st.set_page_config` for basic page layout and title.\n",
    "- `st.sidebar` for displaying \"About\" information and the project disclaimer.\n",
    "- `st.chat_message` and `st.chat_input` for creating a conversational interface.\n",
    "- `st.session_state` for maintaining chat history across user interactions.\n",
    "- `st.markdown` with unsafe_allow_html=True for injecting custom CSS to achieve a thematic visual style.\n",
    "- `st.cache_resource` for caching the RAG chain initialization, ensuring an efficient user experience by avoiding re-loading models and data on every interaction.  \n",
    "\n",
    "Custom image avatars for user and assistant messages to enhance the thematic immersion.  \n",
    "\n",
    "#### Development process:\n",
    "\n",
    "The development of the Streamlit application involved several key stages and iterative refinements:\n",
    "\n",
    "Initial setup and RAG chain integration: The first step was to adapt the RAG chain logic to load the persisted ChromaDB vector store and initialise all LangChain and Google AI components within a function cached by st.cache_resource. This ensured that the potentially time-consuming setup (loading models, vector store) happened only once.  \n",
    "\n",
    "User interface design: A chat-like interface was implemented using st.chat_input for user queries and st.chat_message to display the conversation history. Session state was used to keep track of messages.  \n",
    "\n",
    "Thematic Styling (CSS): Created a CSS style.\n",
    "\n",
    "\n",
    "#### Deployment considerations (Streamlit Cloud):\n",
    "\n",
    "Problem solved (GitHub file size limits): The ChromaDB vector store files (chroma.sqlite3 and data_level0.bin) were initially too large for direct GitHub commits. This was resolved by implementing Git LFS to track these large files, allowing the pre-built database to be included in the repository for Streamlit Cloud deployment.  \n",
    "\n",
    "Problem solved (Dependency and python version conflicts): Several ModuleNotFoundError and TypeError issues (related to langchain_google_genai, protobuf, and distutils) were encountered during deployment attempts. These were systematically resolved by:\n",
    "- Ensuring all necessary LangChain integration packages were explicitly listed in requirements.txt.\n",
    "- Pinning the protobuf library to version 3.20.3 to avoid compatibility issues with pre-generated code in dependencies.\n",
    "- Migrating the local development environment and the Streamlit Cloud app configuration from Python 3.13 to Python 3.11 to resolve the distutils ModuleNotFoundError (as distutils is removed in Python 3.12+ and some dependencies hadn't fully adapted).  \n",
    "\n",
    "Problem solved (SQLite version on Streamlit Cloud): A RuntimeError from ChromaDB indicated an unsupported sqlite3 version on the Streamlit Cloud environment. This was fixed by adding pysqlite3-binary to requirements.txt and including a snippet at the top of app.py to instruct Python to use this newer SQLite version.  \n",
    "\n",
    "Problem solved (API key management): Ensured the GOOGLE_API_KEY was correctly managed using Streamlit's secrets management for secure deployment.  \n",
    "\n",
    "The resulting Streamlit application successfully provides an interactive and thematically styled interface to the Dragon Age RAG chatbot, demonstrating the integration of various LangChain components and handling several common deployment challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a105a777",
   "metadata": {},
   "source": [
    "## Project Reflection\n",
    "\n",
    "This project has resulted in a functional RAG (Retrieval Augmented Generation) chatbot capable of answering questions based on a specific knowledge base sourced from Dragon Age game guides. What follows is a discussion regarding the model's real-world application, challenges, opportunities, and ethical considerations, in accordance with the requirements.\n",
    "\n",
    "### Real-world Use\n",
    "\n",
    "A RAG chatbot like this one has several practical applications, especially within the world of gaming. The primary benefit is offering immediate and contextual information to players, which significantly enhances the user experience.  \n",
    "\n",
    "Integrated in-game help: Instead of pausing the game to search an external wiki, a player could ask a question directly via an in-game overlay or a companion app. This reduces friction and helps the player remain immersed in the game world.  \n",
    "Community support: On platforms like Discord or Reddit, such a chatbot could serve as a first line of defense by automatically answering frequently asked questions about the game's lore, mechanics, or specific quests. This frees up time for human moderators to focus on more complex discussions.  \n",
    "Onboarding for new players: Large games like Dragon Age can be overwhelming for beginners. A RAG bot can act as a personal guide, helping the player understand fundamental concepts without revealing spoilers.  \n",
    "\n",
    "### Potential challenges\n",
    "\n",
    "Despite the model's success in this project, there are significant challenges in implementing and maintaining such a system in a real-world environment.  \n",
    "\n",
    "Data maintenance and version control: Most games are continuously updated with patches and expansions (it is not a case with the Dragon Age series because the development on them has stopped... unless a remaster is released in the future). The knowledge base (the PDF files) can become outdated. A robust process for updating the source documents, re-running the chunking and embedding processes, and validating the new information is necessary, which entails an ongoing operational cost.  \n",
    "Handling ambiguous queries: Users rarely ask perfect questions. A query like \"Where do I find the best sword?\" is subjective and lacks the semantic precision required to match a specific text chunk. The model might then fail to find relevant context or retrieve information that doesn't match the user's intent, leading to irrelevant answers.  \n",
    "Inaccuracies in source data: The RAG model is entirely dependent on its knowledge base. If a game guide contains a factual error, the chatbot will present this error as truth. The system lacks the capacity for critical thinking or external fact-checking.  \n",
    "LLM limitations and hallucinations: Even with a strong instruction to only use the provided context, there is a risk that the language model will \"hallucinate\" or misinterpret information, especially if the retrieved chunks are ambiguous or contradictory. The in-game character persona might also inadvertently encourage creativity where fact-based recall is desired.  \n",
    "Scalability: The system built here works well for a single user. Handling thousands of concurrent users would require a transition to a cloud architecture to manage API calls (costs and rate limits) and vector database queries.  \n",
    "\n",
    "### Opportunities\n",
    "\n",
    "Despite the challenges, the commercial and user-centric opportunities are significant.  \n",
    "\n",
    "Commercialization: Game developers could offer such a chatbot as a premium feature in an official app or as part of a subscription service. It could also be used as a marketing tool on the game's website to increase engagement.  \n",
    "Personalization: A more advanced version could connect to a player's save file. By knowing the player's progress, the chatbot could provide tailored advice and avoid spoilers for quests or areas the player has not yet discovered.  \n",
    "Accessibility: For players with certain disabilities, a voice- or text-based chat interface might be a more accessible way to obtain information compared to navigating complex menus or websites.  \n",
    "\n",
    "### Ethical Considerations\n",
    "\n",
    "Finally, it is important to reflect on the ethical implications.  \n",
    "\n",
    "Bias in source material: Game guides and lore books are written by people and may contain cultural or other forms of bias. The RAG system will inherit and amplify these biases without question, presenting them as objective fact.  \n",
    "Dissemination of misinformation: As previously mentioned, errors in the source data will lead to incorrect answers. An authoritative and convincing persona might cause users to trust this incorrect information, which could lead to frustration and the spread of incorrect lore within the game's community.  \n",
    "Data privacy and logging: What happens to the questions users ask? If they are logged to improve the system, there must be a clear policy on how the data is stored, who has access to it, and for how long it is kept. User anonymity and privacy must be protected.  \n",
    "Persona and manipulation: A well-written persona can create a form of emotional connection. There is an ethical responsibility not to design these personas in a way that could be manipulative or misleading to vulnerable users.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
